%%
%% This is file `sample-acmlarge.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `all,journal,bibtex,acmlarge')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-acmlarge.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%%
%% The first command in your LaTeX source must be the \documentclass
%% command.
%%
%% For submission and review of your manuscript please change the
%% command to \documentclass[manuscript, screen, review]{acmart}.
%%
%% When submitting camera ready or to TAPS, please change the command
%% to \documentclass[sigconf]{acmart} or whichever template is required
%% for your publication.
%%
%%
\documentclass[acmlarge]{acmart}

%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{acmlicensed}
\copyrightyear{2018}
\acmYear{2018}
\acmDOI{XXXXXXX.XXXXXXX}


%%
%% These commands are for a JOURNAL article.
\acmJournal{POMACS}
\acmVolume{37}
\acmNumber{4}
\acmArticle{111}
\acmMonth{8}

%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}


%%
%% end of the preamble, start of the body of the document source.


\usepackage[english]{babel}
\usepackage{amsmath, amsfonts, amsthm}
\usepackage{graphicx}
\usepackage{float}
\usepackage{multicol}


\begin{document}
\tableofcontents
\newpage
\section{Learning to Detect Roads}
\subsection{Unsupervised Pretraining}  

\raggedright Traditionally neural networks have been initialized with small random weights. However, it has recently been shown that using an unsupervised learning procedure to initialize the weights can significantly improve the performance of neural networks. Using such an initialization procedure has been referred to as pretraining.
\newline

\raggedright A  set  of  inclusion  and  exclusion  criteria  was  ascertained  as  competency  factors  to  identify  previous  studies  and  subjects  based  on  the  purpose  of  this  work.  The exclusion factors  were as follow.
\noindent

\begin{itemize}
    \item[-] The full text of the papers was not provided by publishers;
    \item[-] Remote sensing images were not used in the he papers;
    \item[-] Peer‐reviewed papers, such as conferences and journals;
    \item[-] Articles written in English;
\end{itemize}

\raggedright Although the methods utilized for road extraction used different data, this study can provide the following important outcomes 

\noindent
\begin{enumerate}
\item The capabilities of deep learning methods for road extraction are more effective than those of regular approaches
\item The low efficiency of the proposed methods in terms of data quality, training dataset, and model hyperparameters is presented Table(\ref{2})
\item Occlusions, such as shadows, cars, and buildings, are similar to road features, such as colors, reflectance, and patterns. Road extraction remains challenging owing to such issues Figure(\ref{1})
\end{enumerate}

\begin{figure}[H]
    \centering
    \includegraphics[width=100mm]{image.jpg}
    \caption{The pixel resolution of most }
    \label{1}
\end{figure}

\begin{table}[h!]
    \begin{center}
        \caption{The confusion matrix is a detailed}
        \label{2}
          \begin{tabular}{|p{3cm}|p{4cm}|p{4cm}|}
          \hline
           Year&Channel&Resolution\\
          \hline
          2017&N,R,G,B&15cm/px\\
          \hline
          2018&N,R,G,B&10cm/px\\
          \hline
          \end{tabular}
   \end{center}
\end{table}

\raggedright where Z is a normalizing constant and the energy E$(v, h)$ 
formula(\ref{3}) is: \newline
\begin{equation}
E(V,h)=\sum\limits
        _{i=1} 
V^2_i -\left( \sum\limits_{k=1} b_k h_k + \sum\limits_{i=1} w_i V_i h_k \right). \label{3}
\end{equation}

\raggedright In this work, we use Mean Squared Error (MSE) formula(\ref{4}) as the loss function:

\begin{equation}
             ^N
\frac{1}{N}\sum\limits V^2_i|| Net(I_i;W)-S_i||^2,
           _{i=1}
\label{4}  \text{where: $N$ – is the number}
\end{equation}

 of the training samples,     
 $W$  –  stochastic gradient descent.
 
 \subsection{Adding Rotations}
\raggedright When training the neural network f we found that it is useful to rotate each training case by a random angle each time it is processed. Since many cities have large areas where the road network forms a grid, training on data without rotations will result in a model that is better at detecting roads at certain orientations. By randomly rotating the training cases the resulting models do not favor roads in any particular orientation.
\newline
We propose a large-scale learning approach to road detection that addresses all three problems as follows
 \begin{itemize}
    \item[-] We use synthetic road/non-road labels that we generate from readily available vector road maps. This allows us to generate much larger labelled datasets than the ones that have been used in the past;
    \item[-]By using neural networks implemented on a graphics processor as our predictors we are able to efficiently learn a large number of features and use a large context for making predictions;
\end{itemize}

One major study of our work is the effectiveness of training multi-spectral data in image recognition.
\begin{enumerate}
\item We simultaneously investigate the impact of using models with different complexities Figure(\ref{5});
\item Aerial farmland images contain annotations with vastly different sizes;
\item In order to justify our choice of using 512 × 512 windows to construct the Agriculture-Vision dataset, we additionally generate two versions of the dataset with different window sizes;
\end{enumerate}

\begin{figure}
        \centering
        \includegraphics[width=100mm]{image2.jpg}
        \caption{The pixel resolution}
        \label{5}
    \end{figure}

\raggedright The dropout method operates on the fully connected layers to avoid overfitting because a fully connected layer usually contains a large number of parameters formula(\ref{6})

\begin{equation}
\label{6}
P(y_i)=\frac{e^Y_i}{ \sum\limits^c_{k=1} e^y_i}
\end{equation}


\end{document}
\endinput
%%
%% End of file `sample-acmlarge.tex'.
